{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  3LC x AWS Cloud @ UT Dallas Hackathon\n",
        "## Chihuahua vs Muffin Classification Challenge\n",
        "\n",
        "**Welcome!** This starter notebook will help you:\n",
        "1. Load and explore the dataset\n",
        "2. Train a baseline CNN model (~83% accuracy)\n",
        "3. Integrate with 3LC for data-centric AI\n",
        "4. Learn the Train-Fix-Retrain workflow\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“‹ Your Task\n",
        "Use data-centric AI techniques to **improve model accuracy** beyond the baseline. Good luck! ðŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Data science\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n",
        "\n",
        "# Progress bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Paths and Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project paths (relative to notebook location)\n",
        "BASE_PATH = Path('../')\n",
        "DATA_PATH = BASE_PATH / 'data'\n",
        "TRAIN_PATH = DATA_PATH / 'train'\n",
        "TEST_PATH = DATA_PATH / 'test'\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "IMG_SIZE = 128\n",
        "\n",
        "# Display paths\n",
        "print(\"ðŸ“‚ Project Structure:\")\n",
        "print(f\"  Base:  {BASE_PATH.absolute()}\")\n",
        "print(f\"  Data:  {DATA_PATH.absolute()}\")\n",
        "print(f\"  Train: {TRAIN_PATH.absolute()}\")\n",
        "print(f\"  Test:  {TEST_PATH.absolute()}\")\n",
        "\n",
        "# Check if data exists\n",
        "if TRAIN_PATH.exists():\n",
        "    print(\"\\nâœ… Training data found!\")\n",
        "else:\n",
        "    print(\"\\nâŒ Training data not found! Please download the dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "full_dataset = datasets.ImageFolder(root=str(TRAIN_PATH), transform=train_transform)\n",
        "\n",
        "# Split into train/validation (80/20)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], \n",
        "                                          generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"ðŸ“¦ Dataset Split:\")\n",
        "print(f\"   Training:   {len(train_dataset)} samples\")\n",
        "print(f\"   Validation: {len(val_dataset)} samples\")\n",
        "print(f\"   Classes: {full_dataset.classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Baseline CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"Baseline CNN for binary classification\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 16 * 16)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ðŸ§  Model created with {total_params:,} parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "def validate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "print(\"âœ… Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"ðŸš€ Starting training...\\n\" + \"=\"*60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        # Create models directory if it doesn't exist\n",
        "        os.makedirs('../models', exist_ok=True)\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'val_acc': val_acc}, \n",
        "                  '../models/best_model.pth')\n",
        "        print(f\"  âœ… New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\\nâœ… Training complete! Best validation accuracy: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "ax2.plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
        "ax2.axhline(y=83, color='r', linestyle='--', label='Baseline (83%)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Final Results:\")\n",
        "print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "if best_val_acc > 83:\n",
        "    print(f\"   âœ… You beat the baseline by {best_val_acc - 83:.2f}%!\")\n",
        "else:\n",
        "    print(f\"   ðŸ’ª Try to improve by {83 - best_val_acc:.2f}% to beat baseline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 3LC Integration (Advanced)\n",
        "\n",
        "**Your Next Steps:**\n",
        "1. Install 3LC: `pip install tlc`\n",
        "2. Sign up at [3lc.ai](https://3lc.ai)\n",
        "3. Register your dataset as a 3LC Table\n",
        "4. Extract embeddings from your model\n",
        "5. Use 3LC Dashboard to explore and identify problematic samples\n",
        "6. Apply data-centric improvements (remove outliers, fix labels, etc.)\n",
        "7. **Retrain and iterate!**\n",
        "\n",
        "### Train-Fix-Retrain Loop:\n",
        "- **Analyze:** Use 3LC to find misclassified/ambiguous samples\n",
        "- **Fix:** Remove bad data, add augmentation, rebalance classes\n",
        "- **Retrain:** Train on improved dataset\n",
        "- **Compare:** Track improvements in 3LC table revisions\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Your Turn to Improve! ðŸŽ¯\n",
        "\n",
        "### Ideas:\n",
        "1. **Use Transfer Learning:** Try ResNet18 or EfficientNet\n",
        "2. **Better Augmentation:** Add more aggressive transforms\n",
        "3. **Data-Centric:** Use 3LC to identify and fix data issues\n",
        "4. **Hyperparameter Tuning:** Adjust learning rate, batch size, etc.\n",
        "\n",
        "**Goal:** Beat 83% baseline accuracy using data-centric AI!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE - Experiment and improve!\n",
        "# Example: Try transfer learning with ResNet\n",
        "\n",
        "# from torchvision.models import resnet18\n",
        "# model_improved = resnet18(pretrained=True)\n",
        "# model_improved.fc = nn.Linear(model_improved.fc.in_features, 2)\n",
        "# model_improved = model_improved.to(device)\n",
        "# ... train with improved model\n",
        "\n",
        "print(\"ðŸ’¡ Add your improvements here and retrain!\")\n",
        "print(\"Remember to use 3LC for data-centric insights!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
